---
title: "Data Science Capstone - Milestone Report"
author: "Harshad B."
date: "23/09/2020"
output: html_document
---

### Introduction

This is the Milestone Report for Data Science: Capstone Project - Week 2 authorized by John Hopkins University offered through Coursera. The objective of this report is to develop an understanding of the dataset provided as part of the project. These preliminary insights into the data will help us when we build the next work prediction mode. We will perform basic exploratory data analysis, and take a look at this dataset.

The model will be trained using a unified document corpus compiled from the
following three sources of text data:

1. Blogs
1. News
1. Twitter

The provided text data are provided in four different languages. This project
will only focus on the English corpora.

### Tasks to be accomplished

The following tasks are to be accomplished in this milestone report.

1. **Task 1:**
  * Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.
  * Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

2. **Task 2:**
  * Build basic n-gram model for predicting the next word based on the previous 1, 2, or 3 words.
  * Build a model to handle unseen n-grams - in some cases people will want to type a combination of words that does not appear in the corpora. Build a model to handle cases where a particular n-gram isnâ€™t observed

### Load the Libraries and Data

First, we will load all the necessary libraries.

```{r load-libraries}
suppressMessages(suppressWarnings(library(NLP)))
suppressMessages(suppressWarnings(library(tm)))
suppressMessages(suppressWarnings(library(RColorBrewer)))
suppressMessages(suppressWarnings(library(wordcloud)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(stringi)))
suppressMessages(suppressWarnings(library(RWeka)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(ngram)))
suppressMessages(suppressWarnings(library(quanteda)))
suppressMessages(suppressWarnings(library(gridExtra)))
```

Now, loading the downloaded training data.

```{r load-data, echo = TRUE}
# blogs
blogs_file <- file("C:/Users/harsh/Desktop/en_US/en_US.blogs.txt","r")
suppressWarnings(blogs_lines <- readLines(blogs_file, encoding="UTF-8"))
close(blogs_file)
# news
news_file <- file("C:/Users/harsh/Desktop/en_US/en_US.news.txt","r")
suppressWarnings(news_lines <- readLines(news_file, encoding="UTF-8"))
close(news_file)
# twitter
twitter_file <- file("C:/Users/harsh/Desktop/en_US/en_US.twitter.txt","r")
suppressWarnings(twitter_lines <- readLines(twitter_file, encoding="UTF-8"))
close(twitter_file)
```

Here's an overall summary of the data.

```{r data-summary}
summary <- sapply(list(blogs_file,news_file,twitter_file),function(x) summary(stri_count_words(x))[c('Min.','Mean','Max.')])
rownames(summary) <- c('Min','Mean','Max')
stats <- data.frame(
  FileName=c("en_US.blogs","en_US.news","en_US.twitter"),      
  t(rbind(sapply(list(blogs_file,news_file,twitter_file),stri_stats_general)[c('Lines','Chars'),],  Words = sapply(list(blogs_file,news_file,twitter_file),stri_stats_latex)['Words',], summary)))
head(stats)
```